{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Training Data, Add Labels, Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     labels  label_id\n",
      "0               alt.atheism         1\n",
      "1             comp.graphics         2\n",
      "2   comp.os.ms-windows.misc         3\n",
      "3  comp.sys.ibm.pc.hardware         4\n",
      "4     comp.sys.mac.hardware         5\n",
      "5            comp.windows.x         6\n",
      "6              misc.forsale         7\n",
      "7                 rec.autos         8\n",
      "8           rec.motorcycles         9\n",
      "9        rec.sport.baseball        10\n",
      "   doc_id  word_id  count\n",
      "0       1        1      4\n",
      "1       1        2      2\n",
      "2       1        3     10\n",
      "3       1        4      4\n",
      "4       1        5      2\n",
      "5       1        6      1\n",
      "6       1        7      1\n",
      "7       1        8      1\n",
      "8       1        9      3\n",
      "9       1       10      9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11239</th>\n",
       "      <td>11240</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11240</th>\n",
       "      <td>11241</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11241</th>\n",
       "      <td>11242</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11242</th>\n",
       "      <td>11243</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11243</th>\n",
       "      <td>11244</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11244</th>\n",
       "      <td>11245</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11245</th>\n",
       "      <td>11246</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11246</th>\n",
       "      <td>11247</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11247</th>\n",
       "      <td>11248</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11248</th>\n",
       "      <td>11249</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11249</th>\n",
       "      <td>11250</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11250</th>\n",
       "      <td>11251</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11251</th>\n",
       "      <td>11252</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11252</th>\n",
       "      <td>11253</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253</th>\n",
       "      <td>11254</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11254</th>\n",
       "      <td>11255</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11255</th>\n",
       "      <td>11256</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11256</th>\n",
       "      <td>11257</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11257</th>\n",
       "      <td>11258</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11258</th>\n",
       "      <td>11259</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11259</th>\n",
       "      <td>11260</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11260</th>\n",
       "      <td>11261</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11261</th>\n",
       "      <td>11262</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11262</th>\n",
       "      <td>11263</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11263</th>\n",
       "      <td>11264</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11264</th>\n",
       "      <td>11265</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11265</th>\n",
       "      <td>11266</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11266</th>\n",
       "      <td>11267</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11267</th>\n",
       "      <td>11268</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11268</th>\n",
       "      <td>11269</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11269 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_id  label_id\n",
       "0           1         1\n",
       "1           2         1\n",
       "2           3         1\n",
       "3           4         1\n",
       "4           5         1\n",
       "5           6         1\n",
       "6           7         1\n",
       "7           8         1\n",
       "8           9         1\n",
       "9          10         1\n",
       "10         11         1\n",
       "11         12         1\n",
       "12         13         1\n",
       "13         14         1\n",
       "14         15         1\n",
       "15         16         1\n",
       "16         17         1\n",
       "17         18         1\n",
       "18         19         1\n",
       "19         20         1\n",
       "20         21         1\n",
       "21         22         1\n",
       "22         23         1\n",
       "23         24         1\n",
       "24         25         1\n",
       "25         26         1\n",
       "26         27         1\n",
       "27         28         1\n",
       "28         29         1\n",
       "29         30         1\n",
       "...       ...       ...\n",
       "11239   11240        20\n",
       "11240   11241        20\n",
       "11241   11242        20\n",
       "11242   11243        20\n",
       "11243   11244        20\n",
       "11244   11245        20\n",
       "11245   11246        20\n",
       "11246   11247        20\n",
       "11247   11248        20\n",
       "11248   11249        20\n",
       "11249   11250        20\n",
       "11250   11251        20\n",
       "11251   11252        20\n",
       "11252   11253        20\n",
       "11253   11254        20\n",
       "11254   11255        20\n",
       "11255   11256        20\n",
       "11256   11257        20\n",
       "11257   11258        20\n",
       "11258   11259        20\n",
       "11259   11260        20\n",
       "11260   11261        20\n",
       "11261   11262        20\n",
       "11262   11263        20\n",
       "11263   11264        20\n",
       "11264   11265        20\n",
       "11265   11266        20\n",
       "11266   11267        20\n",
       "11267   11268        20\n",
       "11268   11269        20\n",
       "\n",
       "[11269 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add labels, print dataframes to check\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_map = pd.read_csv('train.map', header = None, delimiter=' ', names= ['labels', 'label_id'])\n",
    "train_data = pd.read_csv('train.data', header = None, delimiter=' ', names=['doc_id', 'word_id', 'count'] )\n",
    "train_label = pd.read_csv('train.label', header = None, delimiter=' ', names= ['label_id'])\n",
    "train_label=train_label.reset_index()\n",
    "train_label.columns = ['doc_id', 'label_id']\n",
    "#add one to the doc ID so that there are no zeroes\n",
    "train_label['doc_id']=train_label['doc_id']+1\n",
    "print train_map[:10]\n",
    "print train_data[:10]\n",
    "train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Training Data Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Total Doc Count & Docs Per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_joined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-00d6f5c7f4f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Get total document count by searching the unique values, this could also be done by taking max of the ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtotal_doc_count\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_data_joined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Count number of docs for each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdocs_per_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data_joined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_joined' is not defined"
     ]
    }
   ],
   "source": [
    "#Get total document count by searching the unique values, this could also be done by taking max of the ID \n",
    "total_doc_count= train_data_joined['doc_id'].unique().shape\n",
    "\n",
    "# Count number of docs for each class\n",
    "docs_per_class=train_data_joined.groupby('label_id').doc_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Pi for Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate Pi for Each Class, this is the number of documents in a class/total number of docs\n",
    "df_docs_frac_per_class=np.log(train_data_joined.groupby('label_id').doc_id.nunique()/total_doc_count)\n",
    "pi_values=df_docs_frac_per_class.reset_index()\n",
    "pi_values.columns=['label_id','log_pi']\n",
    "#pi_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Total Words Per Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_per_class=train_data_joined[['label_id','count']].groupby('label_id').sum().reset_index()\n",
    "words_per_class.columns=['label_id','words_per_label']\n",
    "#words_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Total Words Per Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_words_per_doc=train_data_joined.groupby(['label_id','word_id'])['count'].sum().reset_index()\n",
    "sum_words_per_doc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Training Data Probability Distribution (Pj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SACHIN: THIS IS WHERE I'M HAVING ISSUES, I NEED TO THE PANDAS DATAFRAME UNSTACKED AND THE REST SHOULD WORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#count of a specific word in the same class/total number of words in each class\n",
    "#Alpha value for smoothing\n",
    "a = 0.001\n",
    "\n",
    "#Calculate probability of each word based on the class\n",
    "pb_j=train_data_joined.groupby('label_id')\n",
    "pb_ij=train_data_joined[['label_id','word_id','count']].groupby(['label_id','word_id'])\n",
    "prob =  (pb_ij['count'].sum() + 1) / (pb_j['count'].sum() + 61188) \n",
    "k=prob.unstack()\n",
    "k2=k.fillna(1.0/61188)\n",
    "k3=k2.stack()\n",
    "prob_distr=k3.reset_index()\n",
    "prob_distr.columns=['label_id','word_id','log(prob)']\n",
    "prob_distr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #add one for all of the null values as (1/count+V+1)\n",
    "# for c in range(1,21):\n",
    "#     prob_distr.loc[c,:] = prob_distr.loc[c,:].fillna(a/(pb_j['count'].sum()[c] + 16689))\n",
    "    \n",
    "# #Convert to dictionary for greater speed\n",
    "# prob_distr_dict = prob_distr.to_dict()\n",
    "\n",
    "# #prob_distr=prob_distr.reset_index()\n",
    "# x=prob_distr.unstack\n",
    "# x\n",
    "# ###### I NEED X AS IT IS HERE Reindexed, But I cant save the instance Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_map = pd.read_csv('test.map', header = None, delimiter=' ', names= ['test_labels', 'test_label_id'])\n",
    "test_data = pd.read_csv('test.data', header = None, delimiter=' ', names=['test_doc_id', 'test_word_id', 'test_count'] )\n",
    "test_label = pd.read_csv('test.label', header = None, delimiter=' ', names= ['test_label_id'])\n",
    "test_label=test_label.reset_index()\n",
    "test_label.columns = ['test_doc_id', 'label_id']\n",
    "print test_map.head(5)\n",
    "print test_data.head(5)\n",
    "test_label.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_label['test_doc_id']=test_label['test_doc_id']\n",
    "merged_test_data = pd.merge( test_label, test_data, on='test_doc_id', how=\"inner\")\n",
    "merged_test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Single Test Document for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=merged_test_data.loc[merged_test_data['test_doc_id']==1];\n",
    "df.columns = df.columns.str.replace('test_word_id','word_id')\n",
    "#sum_words_per_doc=df.groupby(['word_id','doc_id'])['count'].sum().reset_index()\n",
    "sum_words_per_doc.head(5)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Bayesian Algorithm: Map Probability for a Doc that Belongs to Each Class\n",
    "#1) Match on Doc ID, #2 Pull Out Label ID, Probability Word Belongs to that class, and count of word occurence in document; sort by word ID so you can see the map of each word mapping to each class #3 multiply words to get prior probability #4 sum prior probability over each class (it can be added due to the log). #5 raname so that ID's line up and pie values for each class can be multiplied #6 add up the pi with the prior prob, can add because of the log values #7 Answer gives the probability that the document belongs to each class\n",
    "#Log(Probability(word/class)^N,words)summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_doc_dist= pd.merge(prob_distr,df, on='word_id')\n",
    "single_doc_dist= single_doc_dist[['label_id_x','word_id','log(prob)','test_count']].sort_values(['word_id','label_id_x'])\n",
    "single_doc_dist ['prior_prob']= single_doc_dist['test_count']*single_doc_dist['log(prob)']\n",
    "single_doc_dist=single_doc_dist.groupby(['label_id_x'])['prior_prob'].sum().round(3).reset_index()\n",
    "single_doc_dist.columns=['label_id','prior_prob']\n",
    "single_doc_dist = pd.merge (single_doc_dist,pi_values, on='label_id' )\n",
    "single_doc_dist['probability']=single_doc_dist['prior_prob']+single_doc_dist['log_pi']\n",
    "single_doc_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_doc_dist['probability'].idxmax()+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Function to Classify One Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SingleDoc_NaiveBayesClassifier(doc):\n",
    "    df=merged_test_data.loc[merged_test_data['test_doc_id']==doc];\n",
    "    df.columns = df.columns.str.replace('test_word_id','word_id')\n",
    "    ###### Copy below this line, above is for the function\n",
    "    single_doc_dist= pd.merge(prob_distr,df, on='word_id')\n",
    "    single_doc_dist= single_doc_dist[['label_id_x','word_id','log(prob)','test_count']].sort_values(['word_id','label_id_x'])\n",
    "    single_doc_dist ['prior_prob']= single_doc_dist['test_count']*single_doc_dist['log(prob)']\n",
    "    single_doc_dist=single_doc_dist.groupby(['label_id_x'])['prior_prob'].sum().round(3).reset_index()\n",
    "    single_doc_dist.columns=['label_id','prior_prob']\n",
    "    single_doc_dist = pd.merge (single_doc_dist,pi_values, on='label_id' )\n",
    "    single_doc_dist['probability']=single_doc_dist['prior_prob']+single_doc_dist['log_pi']\n",
    "    class_rating = single_doc_dist['probability'].idxmax()+1\n",
    "    print class_rating\n",
    "SingleDoc_NaiveBayesClassifier(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup New Classifier Function for All Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sachin, I would like to finish this tomorrow, I cant find the error that is \n",
    "# giving 100% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NaiveBayesClassifier(df):\n",
    "    df_all= df\n",
    "    df_all.columns = df_all.columns.str.replace('test_word_id','word_id')\n",
    "    all_doc_dist= pd.merge(prob_distr,df_all, on=['label_id','word_id'])\n",
    "    all_doc_dist= all_doc_dist[['label_id','word_id','log(prob)','test_count','test_doc_id']].sort_values(['word_id','label_id'])\n",
    "    all_doc_dist ['prior_prob']= all_doc_dist['test_count']*all_doc_dist['log(prob)']\n",
    "    all_doc_dist=all_doc_dist.groupby(['test_doc_id','label_id'])['prior_prob'].sum().reset_index()\n",
    "    all_doc_dist.columns=['test_doc_id','label_id','prior_prob']\n",
    "    all_doc_dist = pd.merge (all_doc_dist,pi_values, on='label_id' )\n",
    "    all_doc_dist['probability']=all_doc_dist['prior_prob']+all_doc_dist['log_pi']\n",
    "    classified_dataframe= all_doc_dist.sort_values('probability', ascending=False).groupby('test_doc_id', as_index=False).first()\n",
    "    classified_dataframe= classified_dataframe[['test_doc_id','label_id']]\n",
    "    return classified_dataframe\n",
    "classified_dataframe=NaiveBayesClassifier(merged_test_data)\n",
    "# print classified_dataframe\n",
    "classified_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy of Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_label.columns = ['doc_id', 'test_label_id']\n",
    "classified_dataframe.columns = ['doc_id', 'predicted_label_id']\n",
    "guess_to_real= pd.merge(classified_dataframe,test_label, on='doc_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guess_to_real['err']=guess_to_real.apply(lambda row:row['predicted_label_id']-row['test_label_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(guess_to_real['err'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_list = pd.read_csv('vocabulary.txt', header = None, delimiter=' ', names= ['vocab'])\n",
    "vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vocab=61188\n",
    "#Vocab*20=1223760\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
