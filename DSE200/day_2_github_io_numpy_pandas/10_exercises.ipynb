{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Week 2 Exercises\n",
    "\n",
    "In this weeks exercises you will use Numpy/Scipy to impliment some numerical algorithms and then you will use Pandas to perform a rudamentary data analysis using the KDD 98 dataset.  Along the way you will use unix/basic python from the first week as well as git to save your work.\n",
    "\n",
    "As a first step we import the libraries we'll use later on.  This allows us to use numpy library calls by prefixing the call with np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import the libraries \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Matrix Manipulations\n",
    "Lets first create a matrix and perform some manipulations of it.\n",
    "\n",
    "Using numpy's matrix data structure, define the following matricies:\n",
    "\n",
    "$$A=\\left[ \\begin{array}{ccc} 3 & 5 & 9 \\\\ 3 & 3 & 4 \\\\ 5 & 9 & 17 \\end{array} \\right]$$\n",
    "\n",
    "$$B=\\left[ \\begin{array}{c} 2 \\\\ 1 \\\\ 4 \\end{array} \\right]$$\n",
    "\n",
    "After this solve the matrix equation:\n",
    "$$Ax = B$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "a = np.array([[3,5,9],[3,3,4],[5,9,17]])\n",
    "b= np.array([[2],[1],[4]])\n",
    "print a\n",
    "print b\n",
    "ainv=inv(a)\n",
    "c= np.dot(a, b)\n",
    "print c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write three functions for matrix multiply $C=AB$ in each of the following styles:\n",
    "\n",
    "1. By using nested for loops to impliment the naive algorithm ($C_{ij}=\\sum_{k=0}^{m-1}A_{ik}B_{kj}$)\n",
    "2. Using numpy's built in martrix multiplication  \n",
    "3. Using Cython\n",
    "\n",
    "The three methods should have the same answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1) By using nested for loops to impliment the naive algorithm\n",
    "result = np.full((10,10),0)\n",
    "for i in range(len(a)):\n",
    "    for j in range(len(b[0])):\n",
    "        for k in range(len(b)):\n",
    "            result[i][j] += a[i][k] * b[k][j]\n",
    "for r in result:\n",
    "    print(r)\n",
    "\n",
    "#2) Using numpy's built in martrix multiplication\n",
    "a= np.full((10, 10), 7)\n",
    "b= np.full((10, 10), 9)\n",
    "c= np.dot(a, b)\n",
    "print c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wish to evaluate the performance of these three methods.  Write a method that given three dmiensions (a,b,c) makes a random a x b and b x c matrix and computes the product using your three functions and reports the speed of each method.\n",
    "\n",
    "After this measure performance of each method for all $a,b,c \\in \\{10,100,1000,10000\\}$ and plot the results.  Is one method always the fastest?  Discuss why this is or is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# just do 10, 100, 1000 with same number of field\n",
    "#a= input ('Enter dimension a:')\n",
    "#b= input ('Enter dimension b:')\n",
    "#c= input ('Enter dimension c:')\n",
    "\n",
    "\n",
    "def tdim_long(a,b,c):\n",
    "    m1= np.random.random((a, b))\n",
    "    m2= np.random.random((b, c))\n",
    "\n",
    "    #First Test\n",
    "    result = np.full((a,c),0)\n",
    "    for i in range(len(m1)):\n",
    "        for j in range(len(m2[0])):\n",
    "            for k in range(len(m2)):\n",
    "                result[i][j] += m1[i][k] * m2[k][j]\n",
    "                \n",
    "def tdim_short(a,b,c):\n",
    "    m1= np.random.random((a, b))\n",
    "    m2= np.random.random((b, c))\n",
    "    m3= np.dot(m1, m2)\n",
    "    \n",
    "    \n",
    "#Speed Test for (10,10,10)\n",
    "% time tdim_long(10,10,10)\n",
    "% time tdim_short(10,10,10)\n",
    "\n",
    "    \n",
    "#Speed Test for (100,100,100)\n",
    "% time tdim_long(100,100,100)\n",
    "% time tdim_short(100,100,100)\n",
    "\n",
    "    \n",
    "#Speed Test for (1000,1000,1000)\n",
    "% time tdim_long(1000,1000,1000)\n",
    "% time tdim_short(1000,1000,1000)\n",
    "\n",
    "#The numpy way is much shorter.  This is because instead of looping through the data, the computeris able to go \n",
    "#right to the source of the data and compute the variables all at once. \n",
    "\n",
    "#%timeit\n",
    "#at=%timeit -n 1 -r 1 -o A.dot(B)\n",
    "#at.best\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS** Now repeat the past two problems but instead of computing the matrix product, compute a matrix's [determinant](http://en.wikipedia.org/wiki/Determinant).  Measure performance for matricies of various sizes and discuss the results.  Determinant may get impractical to calculate for not too huge of matricies, so no need to goto 1000x1000 matricies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comp_deter(l):\n",
    "    m_det=np.random.random((l,l))\n",
    "    deter=np.linalg.det(m_det)\n",
    "    print deter\n",
    "    \n",
    "%time comp_deter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###IO Exercises\n",
    "\n",
    "Below is a map of various datatypes in python that you have come across and their corresponding JSON equivalents.\n",
    "\n",
    "$$Datatypes=\\left[ \\begin{array}{cc} JSON & Python3 \\\\ object & dictionary \\\\ array & list \\\\ string & string \\\\ integer\t& integer \\\\ real number & float \\\\ true & True \\\\ false & False \\\\ null & None  \\end{array} \\right]$$\n",
    "\n",
    "\n",
    "There are atleast two very important python datatypes missing in the above list. \n",
    "Can you find the same?  [list the two mising python datatypes in this markdown cell below]\n",
    "\n",
    "1. Tuple\n",
    "2. Set\n",
    "\n",
    "Now We can save the above map as a dictionary with Key-value pairs \n",
    "1. create a python dictionary named dataypes, having the above map as the Key-value pairs with Python datatypes as values and JSON equivalents as keys.\n",
    "2. Save it as a pickle called datatypes and gzip the same.\n",
    "3. Reload this pickle, and read the file contents and output the data in the following formatted way as given in this example - \"The JSON equivalent for the Python datatype Dictionary is Object\". Output similarly for the rest of the key-value pairs.\n",
    "4. Save this data as a JSON but using Python datatypes as keys and JSON equivalent as values this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1)\n",
    "import cPickle as pickle\n",
    "import gzip\n",
    "import shutil\n",
    "dataypes={}\n",
    "dataypes= {'array':'list','string':'string','integer':'integer','realnumber':'float', 'true':'True', 'false':'False', 'null':'None'}\n",
    "print dataypes\n",
    "\n",
    "\n",
    "#2) Save it as a pickle called datatypes and gzip the same\n",
    "pickle.dump( dataypes, open( 'datatypes.p', 'wb' ))\n",
    "with open('datatypes', 'rb') as f_in, gzip.open('datatypes.gz', 'wb') as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "#3) Reload this pickle, and read the file contents and output the data\n",
    "datatypes = pickle.load( open( 'datatypes.p', 'rb' ) )\n",
    "#for x in datatypes:\n",
    "#    print (x)\n",
    "\n",
    "for keys,values in datatypes.items():\n",
    "    #print \"The JSON equivalent for the Python datatype %d is %d\" (values), (keys)\n",
    "    #print (keys)\n",
    "    #print (values)\n",
    "    print \"keys are\",(keys),\" values are\",(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Pandas Data Analysis\n",
    "Pandas gives us a nice set of tools to work with columnar data (similar to R's dataframe). \n",
    "To learn how to use this it makes the most sense to use a real data set.\n",
    "For this assignment we'll use the KDD Cup 1998 dataset, which can be sourced from http://kdd.ics.uci.edu/databases/kddcup98/kddcup98.html .\n",
    "\n",
    "\n",
    "###Acquiring Data\n",
    "First we pull the README file from the dataset into this notebook via the unix \"curl\" command.  Remember you can hide/minimize output cells via the button on the left of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!curl http://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this README describes several files which may be of use.  In particular there are two more documentation files (DOC and DIC) we should read to get an idea of the data format.  Bring these files into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl http://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98doc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!curl http://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98dic.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wish to download the cup98lrn.zip file and unzip it into a new subdirectory called \"data\".  \n",
    "However, since this file is pretty big we don't want to store it on github.  \n",
    "Luckily git provides the [.gitignore](http://git-scm.com/docs/gitignore) file which allows us to specify files we don't want to put into our git repository.\n",
    "\n",
    "Please do the following steps:\n",
    "\n",
    "1. Add the directory \"data\" to the .gitignore file\n",
    "2. Commit the new .gitignore file\n",
    "3. Create a new directory \"data\"\n",
    "4. Download http://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98lrn.zip into the data directory\n",
    "5. Unzip the cup98lrn.zip (we will only be using the unzipped version, so feel free to remove the zip file)\n",
    "6. Run \"git status\" to show that the data directory is not an untracked file (this indicates it is ignored)\n",
    "\n",
    "**NOTE:** These steps only need to be run once, it is advised you comment all the lines out by putting a # at the start of each line after they have run.  This will save you time in the future when you have to rerun all cells/don't want to spend a few minutes downloading the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1)\n",
    "#!git rm --cashed data \n",
    "#!touch .gitignore\n",
    "#added file manually to gitignore using sublime\n",
    "\n",
    "#2) \n",
    "#!git add .gitignore\n",
    "#!git status\n",
    "#!git commit -m \"this is the gitignore commit\"\n",
    "\n",
    "#3)\n",
    "#!mkdir data\n",
    "\n",
    "#4-5) was this support to be with unix? I did it with the GUI\n",
    "\n",
    "#6)\n",
    "#!git status\n",
    "#Data file is still ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform some basic sanity checks on the data.  Using a combination of unix/basic python answer the following questions:\n",
    "\n",
    "1. How many lines are there?  \n",
    "2. Is the file character seperated or fixed width format?\n",
    "3. Is there a header?  If so how many fields are in it?\n",
    "4. Do all rows have the same number of fields as the header?\n",
    "5. Does anyhting in 1-4 disagree with the readme file or indicate erroneous data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1) \n",
    "#!wc -l data/cup98lrn.txt\n",
    "\n",
    "#2) \n",
    "#!head -1 data/cup98lrn.txt\n",
    "#file is comma seperated \",\" delimited\n",
    "\n",
    "#3)\n",
    "#\"Visual check\"\n",
    "#!head -1 data/cup98lrn.txt\n",
    "#!sed -n '5p' < data/cup98lrn.txt\n",
    "\n",
    "\n",
    "#3)\n",
    "#x = open('data/cup98lrn.txt', 'r')\n",
    "#y= x.readlines()\n",
    "#cdel = y[1].split(',')\n",
    "#print \"Fields in the header=\",len(cdel)\n",
    "\n",
    "\n",
    "#4)  \n",
    "#print \"done\"\n",
    "#learn = pd.read_csv('data/cup98lrn.txt', error_bad_lines=False)\n",
    "#learn.head(n=5)\n",
    "#this tells you exactly which lines are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give answers to questions 1-4 in this markdown cell:\n",
    "\n",
    "1. 95413 rows in the file\n",
    "2. Character Seperated with commas \",\"\n",
    "3. Yes there is a header, with 481 columns\n",
    "4. No some lines have null pointers! \n",
    "5. Yes the readme agrees with the data found, other than the lines that had errors. \n",
    "\n",
    "#README Reference: PKZIP compressed raw LEARNING data set. \n",
    "                Internal name: cup98LRN.txt \n",
    "                File size: 36,468,735 bytes zipped. 117,167,952 bytes \n",
    "                unzipped.\n",
    "                Number of Records: 95412.\n",
    "                Number of Fields: 481.\n",
    "\n",
    "Now load the data file into a pandas data frame called \"learn\".  To save some time, we've loaded the data dictionary into col_types.  \n",
    "\n",
    "Finally split learn into two data frames, learn_y: the targets (two columns described in the documentation) and learn_x: the predictors (everything but the targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import scipy as sp\n",
    "#import pandas as pd\n",
    "#dict_file = open(\"data/dict.dat\")\n",
    "#col_types = [ (x.split(\"\\t\")[0], x.strip().split(\"\\t\")[1]) for x in dict_file.readlines() ]\n",
    "#print col_types\n",
    "learn = pd.read_csv('data/cup98lrn.txt', error_bad_lines=False)\n",
    "learn.head(n=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print learn.columns[1]\n",
    "learn_y = learn[[\"TARGET_B\"]+[\"TARGET_D\"]]\n",
    "learn_y.head(n=25)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn_x=learn \n",
    "del learn_x[\"TARGET_B\"]\n",
    "learn_x.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Summarizing Data\n",
    "Now that we have loaded data into the learn table, we wish to to summarize the data.  \n",
    "Write a function called summary which takes a pandas data frame and prints a summary of each column containing the following:\n",
    "\n",
    "If the column is numeric:\n",
    "\n",
    "1. Mean\n",
    "2. Standard Deviation\n",
    "3. Min/Max\n",
    "4. Number of missing values (NaN, Inf, NA)\n",
    "\n",
    "If the column is non alphabetical:\n",
    "\n",
    "1. Number of distinct values\n",
    "2. Number of missing values (NaN, INF, NA, blank/all spaces)\n",
    "3. The frequency of the 3 most common values and 3 least common values\n",
    "\n",
    "Format the output to be human readable.\n",
    "\n",
    "For example:\n",
    "> Field_1  \n",
    "> mean: 50  \n",
    "> std_dev: 25  \n",
    "> min: 0  \n",
    "> max: 100  \n",
    "> missing: 5\n",
    ">  \n",
    "> Field_2  \n",
    "> distinct_values: 100  \n",
    "> missing: 10  \n",
    ">  \n",
    "> 3 most common:  \n",
    ">   the: 1000  \n",
    ">   cat: 950  \n",
    ">   meows: 900  \n",
    ">  \n",
    "> 3 least common:  \n",
    ">   dogs: 5  \n",
    ">   lizards: 4  \n",
    ">   eggs: 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#one = df.groupby('letter')\n",
    "#mean= np.mean(learn, axis=0)\n",
    "#print learn\n",
    "describe = learn.describe(include='all')\n",
    "print describe\n",
    "\n",
    "\n",
    "#for i in describe\n",
    "#    print \n",
    "#print describe.index\n",
    "#print describe.columns\n",
    "#print describe\n",
    "#use transform to find the average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " ### Pandas analysis on Calit2 data \n",
    "\n",
    "Import data from http://archive.ics.uci.edu/ml/machine-learning-databases/event-detection/CalIt2.data using curl\n",
    "\n",
    "This data comes from the main door of the CalIt2 building at UCI. Observations come from 2 data streams (people flow in and out of the building), over 15 weeks, 48 time slices per day (half hour count aggregates).\n",
    "\n",
    "Attribute Information:\n",
    "1. Flow ID: 7 is out flow, 9 is in flow\n",
    "2. Date: MM/DD/YY\n",
    "3. Time: HH:MM:SS\n",
    "4. Count: Number of counts reported for the previous half hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!curl http://archive.ics.uci.edu/ml/machine-learning-databases/event-detection/CalIt2.data\n",
    "Calit2 = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/event-detection/CalIt2.data', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Labeled Columns\n",
    "Calit2.columns=['InOut','Date','Time','People']\n",
    "print Calit2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Selecting Data ####\n",
    "1. Select all data for the date July 24 2005 having flow id=7. Also output the row count of results \n",
    "2. Select all rows whose count is greater than 5. Sort the result on count in descending order and output the top 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Problem 1\n",
    "mask=((Calit2.Date=='07/24/05')&(Calit2.InOut==7))\n",
    "select_July24_Out = Calit2[mask]\n",
    "print \"Rows:\",len(select_July24_Out)\n",
    "print select_July24_Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Problem 2\n",
    "mask2=(Calit2.People>5)\n",
    "select_above5=Calit2[mask2]\n",
    "sorted_result= select_above5.sort_index(ascending=False,by =[\"People\"])\n",
    "print sorted_result.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply function ####\n",
    "1. For the 10 rows outputted above, use Pandas Apply function to subtract lowest value of the 10 from all of them and then output the average value of the resulting counts\n",
    "2. On the entire data, use apply function to sum all counts with flow_id=9 and date is 07/24/05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=sorted_result.head(n=10)\n",
    "\n",
    "#type(df.People-10)\n",
    "\n",
    "\n",
    "#def subtract_low(x):\n",
    "#        result = {'new_value': x['People']-50}\n",
    "#        return pd.Series(result, name='new')\n",
    "\n",
    "#df.apply(subtract_low)\n",
    "\n",
    "#result= df.apply(subtract_low)\n",
    "\n",
    "#df['Check']= df[:,['People']].apply(subtract_low)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing an Selecting ####\n",
    "Exlain the following\n",
    "\n",
    "1. loc: (LABEL-LOCATION) This is the primary access method in pandas, it used for selection by label when the labels determine the way that you want to access the data, for example you might have ordered numbers as the labels and want to run through the data in the order of the labels rather than the inherent integer (nth) label of each column. It can also be used with a boolean array.\n",
    "\n",
    "2. iloc: (INTEGER_LOCATION) Is similiar to .loc, but it explains the selection position is primarily integer based, so if a column has a string label and you want to access the nth column, than you can use the nth column instead.Also can be used with a boolean array.\n",
    "\n",
    "3. ix: (LABEL-LOCATION with NTEGER LOCATION fallback) this supports integer and label based access, it it useful when you have combined index types that are still ordered in a ehigharchal structure. For example you have [A, AA, BB, B3, B4, C, C, C1, CC],  the computer would intepret the labels first, and then for the C values it would look at location insead of the label becasue they have similiar labels.\n",
    "\n",
    "4. at: this is a \"fast label based scalar accesor\" - I believe this would be used if the labels only had numbers in them. \n",
    "\n",
    "5. iat: provides integer based lookups, fast ineger location scalar accessor. This would be used to access the nth column oranizaed by column number. \n",
    "\n",
    "Highlight the differences by providing usecases where one is more useful than the other\n",
    "\n",
    "Write a function to take two dates as input and return all flow ids and counts in that date range having both the dates inclusive. You can use pandas to_datetime function to convert the date to pandas datetime format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#firstdate=input(\"First date:\")\n",
    "#seconddate=input(\"Second date:\")\n",
    "Calit2.head()\n",
    "\n",
    "maskdates=((Calit2.Date>)'firstdate')&(Calit2.Date<'seconddate')\n",
    "select_dates=Cali2[maskdates]\n",
    "\n",
    "df = Calit2.Date(str)\n",
    "pd.to_datetime(df.day + df.month + df.year, format=\"%d%m%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping ####\n",
    "1. Select data in the month of August 2005 having flow id=7\n",
    "2. Group the data based on date and get the max count per date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask1=((Calit2.Date>'07/31/05')&(Calit2.Date<'09/01/05')&(Calit2.InOut==7))\n",
    "select_August = Calit2[mask1]\n",
    "#print select_August\n",
    "grouped=select_August.groupby('Date')\n",
    "grouped.People.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking, Unstacking ####\n",
    "1. Stack the data with count and flow_id as indexes\n",
    "2. Use reset_index to reset the stacked hierarchy by 1 level. The index then will just be the counts\n",
    "3. Unstack the data to get back original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "stack = Calit2.stack()\n",
    "#stack.index\n",
    "print stack\n",
    "\n",
    "#stack=Calit2.stack()\n",
    "\n",
    "\n",
    "\n",
    "#print stack.stack(level=\"People\")\n",
    "#stack.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas and Matplotlib\n",
    "\n",
    "Plot a histogram of date vs total counts for flow_id=7 and flow_id=9 for the month of July 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select Dates\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "July_mask=((Calit2.Date<'08/01/05')&(Calit2.Date>'06/31/05'))\n",
    "select_July=Calit2[July_mask]\n",
    "stacked_date=select_July.stack()\n",
    "groupbydate=select_July.reset_index().groupby('Date').People.sum()\n",
    "print groupbydate\n",
    "plt.hist(groupbydate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Calit2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
