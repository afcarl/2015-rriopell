{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#OAuth Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will try to scrape twitter data and do a tf-idf analysis on that (src-uwes twitter analysis). We will need OAuth authentication, and we will follow a similar approach as detailed in the yelp analysis notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import oauth2 as oauth\n",
    "import urllib2 as urllib\n",
    "import pickle\n",
    "import json\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now need twitter api access. The following steps as available online will help you set up your twitter account and access the live 1% stream.\n",
    "\n",
    "1. Create a twitter account if you do not already have one.\n",
    "2. Go to https://dev.twitter.com/apps and log in with your twitter credentials.\n",
    "3. Click \"Create New App\"\n",
    "4. Fill out the form and agree to the terms. Put in a dummy website if you don't have one you want to use.\n",
    "5. On the next page, click the \"API Keys\" tab along the top, then scroll all the way down until you see the section \"Your Access Token\"\n",
    "6. Click the button \"Create My Access Token\". You can Read more about Oauth authorization online. \n",
    "\n",
    "Save the details of api_key, api_secret, access_token_key, access_token_secret in your vaule directory and load it in the notebook as shown in yelpSample notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need to define the following variables\n",
    "#api_key = #<get api key> \n",
    "#api_secret = #<get api secret>\n",
    "#access_token_key = #<get your access token key here>\"\n",
    "#access_token_secret = #<get your access token secret here>\n",
    "\n",
    "#defining them right here is not safe. insteadm create a file in a different directory\n",
    "# (I use ~/VaultDSE) and in it put a file called, say, twitterkeys.py whose\n",
    "# content is:\n",
    "#api_key = #<get api key>\n",
    "#api_secret = #<get api secret>\n",
    "#access_token_key = #<get your access token key here>\"\n",
    "#access_token_secret = #<get your access token secret here>\n",
    "#\n",
    "#def getkeys():\n",
    "#    return api_key,api_secret,access_token_key,access_token_secret\n",
    "\n",
    "# then use the following commands\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/RyanRiopelle/Documents/DSE/rriopell/DSE200/day_5_mining_the_Social_web/VaultDSE')\n",
    "import twitterKeys\n",
    "api_key,api_secret,access_token_key,access_token_secret=twitterKeys.getkeys()\n",
    "\n",
    "_debug = 0\n",
    "\n",
    "oauth_token    = oauth.Token(key=access_token_key, secret=access_token_secret)\n",
    "oauth_consumer = oauth.Consumer(key=api_key, secret=api_secret)\n",
    "\n",
    "signature_method_hmac_sha1 = oauth.SignatureMethod_HMAC_SHA1()\n",
    "\n",
    "http_method = \"GET\"\n",
    "\n",
    "http_handler  = urllib.HTTPHandler(debuglevel=_debug)\n",
    "https_handler = urllib.HTTPSHandler(debuglevel=_debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a twitter request method which will use the above user logins to sign, and open a twitter stream request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTwitterStream(url, method, parameters):\n",
    "  req = oauth.Request.from_consumer_and_token(oauth_consumer,\n",
    "                                             token=oauth_token,\n",
    "                                             http_method=http_method,\n",
    "                                             http_url=url, \n",
    "                                             parameters=parameters)\n",
    "\n",
    "  req.sign_request(signature_method_hmac_sha1, oauth_consumer, oauth_token)\n",
    "\n",
    "  headers = req.to_header()\n",
    "\n",
    "  if http_method == \"POST\":\n",
    "    encoded_post_data = req.to_postdata()\n",
    "  else:\n",
    "    encoded_post_data = None\n",
    "    url = req.to_url()\n",
    "\n",
    "  opener = urllib.OpenerDirector()\n",
    "  opener.add_handler(http_handler)\n",
    "  opener.add_handler(https_handler)\n",
    "\n",
    "  response = opener.open(url, encoded_post_data)\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above function to request a response as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<addinfourl at 4360247416 whose fp = <socket._fileobject object at 0x108199ed0>>\n"
     ]
    }
   ],
   "source": [
    "url = \"https://stream.twitter.com/1/statuses/sample.json\"\n",
    "parameters = []\n",
    "response = getTwitterStream(url, \"GET\", parameters)\n",
    "print response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we will test the above function for a sample data provided by twitter stream here -  \n",
    "url = \"https://stream.twitter.com/1/statuses/sample.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which will take a url and return the top 10 lines returned by the twitter stream\n",
    "\n",
    "** Note ** The response returned needs to be intelligently parsed to get the text data which correspond to actual tweets. This part can be done in a number of ways and you are encouraged to try different approaches to parse the response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tweet1: Malaysians, Singaporeans nabbed before leaving for Syria had no prior terror links - Inquirer.n... https://t.co/ETD9sAnPlf #ISIS\n",
      "Tweet2: RT @edwardedark: Most schools are closed in west Aleppo today, due to last 2 days of terrorist rebel shelling that murdered &amp; wounded dozen…\n",
      "Tweet3: RT @PhilGreaves01: Hey Prof, @joshua_landis you're still uncritically promoting fairytales of \"Syria Direct\", entirely funded by USGov. htt…\n",
      "Tweet4: RT TheOnlyMan2Know Why are we here? https://t.co/UpipbDs5hU #Hungary #Syria #Palestine #Israel #Iran #European #Europe\n",
      "Tweet5: ثورة الشام بدأت باسم الله وصمدت بفضل الله ولن تنتصر إلا أن يشاء الله، لا منة عليها لأحد إلا لبارئها\n",
      "أحسنوا ظنكم بالله فحاشاه أن يخيب سعيكم\n",
      "Tweet6: RT @memorte2023: Syrian opposition has agreed in Riyadh meeting to form a committee of 23 members https://t.co/SvPCM9Y0rw #SaudiArabia #Syr…\n",
      "Tweet7: RT @HK2307: Acc to @dianadarke #US needs to grasp important difference bw #IS &amp; AlQaeda in #Syria #Nusra;\n",
      "JN respects minorities https://t.…\n",
      "Tweet8: RT @horrificstory: Syria 2010 vs Syria 2015. What is there left to bomb? https://t.co/NAb35yh0CO\n",
      "Tweet9: RT @felix_carletta: Turkish PM accuses Russia of “ethnic cleansing” in Syria https://t.co/Vw6h7WYji3 #sms #usa #Syria\n",
      "Tweet10: RT @MapsAntique: Original 1807 #map of the World\n",
      "https://t.co/RIpc2NnnSU\n",
      "#art #antiques #giftideas #Christmas #Russia #Putin #Syria https:/…\n"
     ]
    }
   ],
   "source": [
    "url=\"https://api.twitter.com/1.1/search/tweets.json?q=\"\n",
    "searchString=\"Syria\"\n",
    "count=\"200\"\n",
    "\n",
    "def WriteFileTwitterSearch(url, searchString,count):\n",
    "    urlSearch=url+searchString+'&count='+count\n",
    "    parameters = []\n",
    "    response = getTwitterStream(urlSearch, \"GET\", parameters)\n",
    "    a= response.readline() \n",
    "    dic=json.loads(a)\n",
    "    tweets=dic['statuses']\n",
    "    tweetdic={}\n",
    "    twCount=0\n",
    "    for t in tweets:\n",
    "        tweetdic['Tweet%i'%twCount]=t['text']\n",
    "        twCount=twCount+1\n",
    "    #json.dump(tweetdic, open(\"twitterStreamFile.txt\",'wb')) \n",
    "    x=1\n",
    "    while x<11:\n",
    "        print \"Tweet%i:\"%x, tweetdic['Tweet%i'%x]\n",
    "        x=x+1\n",
    "    \n",
    "WriteFileTwitterSearch(url, searchString,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can also request twitter stream data for specific search parameters as follows\n",
    "url= \"https://api.twitter.com/1.1/search/tweets.json?q=\"+search_query\n",
    "#get twitter stream first\n",
    "#this one looks like adictionary in a dictionary, still want to do read loads\n",
    "#do in to statuses, which is a list, inside the list is text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the fetchData function to fetch latest live stream data for following search queries and output the first 5 lines\n",
    "\n",
    "1. \"UCSD\"\n",
    "2. \"Donald Trump\"\n",
    "3. \"Syria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsworthy: read this post by @GPS_UCSD professor @StephanHaggard on #NorthKorea, #Iran, #nuclearweapons https://t.co/4F7P9ex04M\n",
      "https://t.co/docK6cmgzR Depressed from Ochem #UCSD #reddit\n",
      "RT @UC_net: #ClimateChange facts you need to know, courtesy of #UCSD @Scripps_Ocean expert — https://t.co/w2pzhc1nUr #COP21 https://t.co/yq…\n",
      "Coursera Data Science Specializations - John Hopkins Data Sci vs. UCSD Big Data via /r/datascience https://t.co/5gYniwydXw\n",
      "RT @UCSDHealth: Our Comprehensive Stroke Center earns gold from @American_Heart &amp; @American_Stroke! https://t.co/4kRKbIEiJK https://t.co/JO…\n"
     ]
    }
   ],
   "source": [
    "#url = \"https://api.twitter.com/1.1/search/tweets.json?q=\"+\"UCSD\"\n",
    "def fetchDataTwitterSearch(url, searchString,count):\n",
    "    urlSearch=url+searchString+'&count='+count\n",
    "    parameters = []\n",
    "    response = getTwitterStream(urlSearch, \"GET\", parameters)\n",
    "    a= response.readline()\n",
    "    dic=json.loads(a)\n",
    "    dic1=dic['statuses']\n",
    "    for i in dic1:\n",
    "        print i['text']\n",
    "    #return dic1\n",
    "fetchDataTwitterSearch(\"https://api.twitter.com/1.1/search/tweets.json?q=\", \"UCSD\",\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Eating: RT if you think this mozzarella stick would be a better president than Donald Trump. https://t.co/xwXui5MeYr\n",
      "RT @bbcnickrobinson: Petition calling for Donald Trump to be barred from UK now most popular ever on govt website\n",
      "Just putting this out there. I will never vote for Donald Trump. Even against Hillary. Trump is not a republican. #republicansagainsttrump\n",
      "RT @IrishTimes: Dubai firm strips Donald Trump’s image and name from golf complex https://t.co/DQnyhOqbW3 via @IrishTimesSport https://t.co…\n",
      "RT @Independent: Watch Sacha Baron Cohen bring back 'Borat’ just to mock Donald Trump https://t.co/9dowafz654 https://t.co/M8Jk9GhWoU\n"
     ]
    }
   ],
   "source": [
    "fetchDataTwitterSearch(\"https://api.twitter.com/1.1/search/tweets.json?q=\", \"Donald Trump\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @CSIS: US Special Forces alone are not enough to turn the tide against ISIS: https://t.co/yZQjJyrDMN https://t.co/nKD2zwlMlx\n",
      "RT @WeAsk4Justice: @RevolutionSyria: أوقفوا مجرم الحرب بوتين، من ارتكاب الجرائم في سوريا.\n",
      "Stop this Russian mass killer\n",
      "#Syria https://t.co…\n",
      "RT @horrificstory: Syria 2010 vs Syria 2015. What is there left to bomb? https://t.co/NAb35yh0CO\n"
     ]
    }
   ],
   "source": [
    "fetchDataTwitterSearch(\"https://api.twitter.com/1.1/search/tweets.json?q=\", \"Syria\",\"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF###\n",
    "\n",
    "tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.It is among the most regularly used statistical tool for word cloud analysis. You can read more about it online (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "We base our analysis on the following\n",
    "\n",
    "1. The weight of a term that occurs in a document is simply proportional to the term frequency\n",
    "2. The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs\n",
    "\n",
    "For this question we will perform tf-idf analysis o the stream data we retrieve for a given search parameter. Perform the steps below\n",
    "\n",
    "1. use the twitterreq function to search for the query \"syria\" and save the top 200 lines in the file twitterStream.txt\n",
    "2. load the saved file and output the count of occurrences for each term. This will be your term frequency\n",
    "3. Calculate the inverse document frequency for each of the term in the output above.\n",
    "4. Divide the term frequency for each of the term by corresponding inverse document frequency.\n",
    "5. Sort the terms in the descending order based on their term freq/inverse document freq scores \n",
    "6. Print the top 10 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'state', 0.076336076305691211), (u'iraq', 0.076336076305691211), (u'world', 0.081181991353744959), (u'refuge', 0.081181991353744959), (u'assad', 0.081181991353744959), (u'thi', 0.095429728354270965), (u'isi', 0.10937850996316029), (u'amp', 0.14303045961686084), (u'syria', 0.2858680416727703), (u'http', 0.79206674534044896)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#text_file = open(\"twitterStreamFile.txt\", \"r\")\n",
    "dict_file = json.load (open(\"twitterStreamFile.txt\") )\n",
    "token_dict={}\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_tokens(input_text):\n",
    "    text = input_text.encode('ascii','ignore')\n",
    "    lowers = text.lower()\n",
    "    no_punctuation = lowers.translate(None, string.punctuation)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "    #print token\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "for key in dict_file:\n",
    "    text = dict_file[key].encode('ascii','ignore')\n",
    "    lowers = text.lower()\n",
    "    no_punctuation = lowers.translate(None, string.punctuation)\n",
    "    token_dict[key] = no_punctuation\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(token_dict.values())\n",
    "\n",
    "text_file = json.load (open(\"twitterStreamFile.txt\") )\n",
    "combined= unicode.join(u'',text_file.values())\n",
    "\n",
    "str = combined \n",
    "response = tfidf.transform([str])\n",
    "#print response\n",
    "\n",
    "answerlist=[]\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "for col in response.nonzero()[1]:\n",
    "    word_value = (feature_names[col], response[0, col])\n",
    "    answerlist.append(word_value)\n",
    "    #print feature_names[col], ' - ', response[0, col]\n",
    "\n",
    "top10=sorted(answerlist, key=lambda student: student[1])\n",
    "print top10[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
